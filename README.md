# ProjectAuditAgent ğŸ•µï¸â€â™‚ï¸

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-green.svg)](https://fastapi.tiangolo.com/)
[![MCP Ready](https://img.shields.io/badge/MCP-Ready-purple.svg)](https://modelcontextprotocol.io/)

**Production-ready Model Context Protocol (MCP) server for deep Python/FastAPI project analysis.**

ProjectAuditAgent performs AST-based code analysis to detect duplicates, dead code, efficiency issues, and security risks, generating actionable markdown reports with **realistic scores** and **comprehensive insights**.

---

## ğŸš€ Features

### **12 Extensible Analysis Tools**
| Tool | Description |
|------|-------------|
| **ğŸ“ Structure** | Directory tree visualization and file statistics |
| **ğŸ—ï¸ Architecture** | Mermaid dependency graphs with subgraph grouping |
| **ğŸ­ Duplication** | 6-line block hashing to detect code duplication |
| **â˜ ï¸ Dead Code** | Unused functions, classes, and imports (Vulture) |
| **âš¡ Efficiency** | Cyclomatic complexity analysis (Radon) |
| **ğŸ§¹ Cleanup** | Cache/temp files detection with size tracking |
| **ğŸ”’ Bandit** | Security vulnerability scanning |
| **ğŸ”‘ Secrets** | Credential detection using `detect-secrets` |
| **ğŸ“‹ Ruff** | Fast Python linter for code quality |
| **ğŸ” Pip-Audit** | Dependency vulnerability checking |
| **âœ… Tests** | Coverage analysis with Unit/Integration/E2E breakdown |
| **ğŸ“ Git** | Recent changes, commit tracking & branch status |

### **Production Capabilities**
* **âœ¨ Agentic Dependency Installation:** AI automatically detects missing tools and asks user permission to install
* **ğŸ“Š Realistic Scoring Algorithm:** Exponential penalties for low coverage (9% = -40 points, not -10!)  
* **ğŸ“‹ Tool Execution Summary:** Comprehensive table showing status of all 12 tools at a glance
* **ğŸ§ª Test Type Detection:** Automatically categorizes tests as Unit, Integration, or E2E
* **ğŸ”„ Recent Changes Tracking:** Shows last commit, author, time, and uncommitted changes
* **â±ï¸ Timeout Protection:** All subprocess calls protected with timeouts to prevent hangs
* **ğŸ¯ Smart Filtering:** Automatically excludes `.venv`, `node_modules`, and build artifacts
* **ğŸ”— Pako Compression:** Mermaid graphs compressed for reliable link generation
* **ğŸ›¡ï¸ Auto-Fix with Safety:** Git dirty check prevents mixing uncommitted changes
* **ğŸ”’ 100% Local:** No code leaves your machine

---

## ğŸ§  Architecture Flow

```mermaid
graph TD
    User["ğŸ‘¤ User / AI Agent"] -->|1. Request Audit| API["ğŸ“¡ MCP Server API"]
    
    subgraph "Orchestration Layer"
        API --> Agent["ğŸ¤– Analyzer Agent"]
        Agent -->|2. Dispatch| Tools
    end
    
    subgraph "Tool Execution Engine"
        direction TB
        Tools{"ğŸ› ï¸ Analysis Tools"}
        
        Tools -->|Static Analysis| AST["AST Parsers"]
        AST --> Structure & Architecture
        AST --> Complexity & DeadCode
        
        Tools -->|Subprocess| Ext["External Runners"]
        Ext --> Security["Bandit"]
        Ext --> Secrets["Detect-Secrets"]
        Ext --> Ruff["Ruff Linter"]
        
        Tools -->|Environment Aware| Env["Smart Venv Detector"]
        Env -->|Find python| Venv{"Found Venv?"}
        Venv -->|Yes| PyTest["Run pytest-cov (Target Env)"]
        Venv -->|No| SysTest["Run pytest (System Env)"]
    end
    
    subgraph "Reporting"
        Structure & Architecture & Complexity & DeadCode --> Agg["ğŸ“Š Result Aggregator"]
        Agg -->|3. Score| Score["ğŸ’¯ Scoring Algorithm"]
        Score -->|4. Generate Markdown| MD["ğŸ“ Final Report"]
    end
    
    MD -->|5. Return Context| User
    
    style User fill:#f9f,stroke:#333,stroke-width:2px
    style Agent fill:#bbf,stroke:#333,stroke-width:2px
    style MD fill:#ff9,stroke:#333,stroke-width:2px
```

---

## ğŸ“¦ Installation & Setup

### 1. Environment Setup
```bash
# Clone and enter repo
git clone https://github.com/Amitro123/mcp-python-auditor.git
cd mcp-python-auditor

# Create venv
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Install Analysis Tools
```bash
# Required for full functionality
pip install bandit detect-secrets vulture radon ruff pip-audit pytest pytest-cov

# Or let the AI install them for you! (Agentic flow)
# Just run the server and it will detect missing tools automatically
```

### 3. Verify Installation
```bash
# Run the verification script
python verify_tools.py

# Should show: âœ… All tools are properly installed!
```

### 4. Run the MCP Server
You have two entry points depending on your needs:

**Option A: FastMCP (Recommended for Claude Desktop/Cursor)**
```bash
python mcp_fastmcp_server.py
# Or with fastmcp CLI:
fastmcp dev mcp_fastmcp_server.py
```

**Option B: FastAPI Server (For API usage)**
```bash
uvicorn app.main:app --reload
```

---

## ğŸ”Œ MCP Configuration (Claude/Cursor)

To use this tool with Claude Desktop or Cursor, add the following to your config file:

**File:** `claude_desktop_config.json`  
**Location:**
- **Mac:** `~/Library/Application Support/Claude/`
- **Windows:** `%APPDATA%\Claude\`

```json
{
  "mcpServers": {
    "project-audit": {
      "command": "python",
      "args": ["C:\\absolute\\path\\to\\mcp-python-auditor\\mcp_fastmcp_server.py"]
    }
  }
}
```

> **Note:** Use absolute paths and double backslashes (`\\`) on Windows.

**ğŸ“– For detailed MCP setup guide, see:** [docs/MCP_USER_GUIDE.md](docs/MCP_USER_GUIDE.md)

---

## ğŸ¯ Usage Examples

Once connected to Claude, you can use natural language to trigger tools.

### 1. Full Audit
```
"Run a full audit on C:/Projects/MyApp and generate a report."
```

**What it does:**
- Runs all 12 analysis tools in parallel
- Generates a scored report (0-100)
- Saves to `reports/FULL_AUDIT_<id>.md`
- Returns markdown content to AI

### 2. Missing Dependencies? No Problem!
```
AI: "I need to run an audit but tools are missing. Would you like me to install them?"
You: "Yes, install them"
AI: [Calls install_dependencies tool]
AI: "âœ… Installation successful! Running audit now..."
```

### 3. Architecture Review
```
"Analyze the project architecture and show me the dependency graph."
```

**What it does:**
- Parses Python imports using AST
- Groups modules into subgraphs by directory
- Generates Mermaid diagram with compressed link

### 4. Auto-Fix (Safe Code Cleanup)
```
"Run auto-fix with confirm=True."
```

**What it does:**
1. âœ… Checks for uncommitted changes (aborts if dirty)
2. ğŸ“¦ Creates backup zip
3. ğŸ—‘ï¸ Deletes cache directories
4. ğŸ¨ Runs `ruff check --fix` and `ruff format`
5. ğŸ“ Writes `FIX_LOG.md`
6. ğŸŒ¿ Creates new branch and commits

---

## ğŸ“Š Scoring Algorithm

The score (0-100) uses **strict, realistic weights** to avoid false positives:

| Category | Max Penalty | How It's Calculated |
|----------|-------------|---------------------|
| **Security** | -30 points | Bandit issues (-20), Secrets found (-10) |
| **Testing** | -40 points | **Exponential:** <20% = -40, <50% = -25, <80% = -10 |
| **Quality** | -20 points | Duplicates (-15 max), Dead code (-5 max) |
| **Complexity** | -10 points | High-complexity functions (-2 each, -10 max) |

### Before vs. After Fix

**âŒ Old Algorithm (Broken):**
- Project with 9% coverage + 78 duplicates = **90/100** ğŸŸ¢ (WRONG!)

**âœ… New Algorithm (Realistic):**
- Same project = **45/100** ğŸ”´ (Honest assessment!)

**Score Grades:**
- ğŸŸ¢ **80-100:** Excellent (strict threshold)
- ğŸŸ¡ **60-79:** Good  
- ğŸ”´ **0-59:** Needs Improvement

---

## ğŸ“ Report Features

Every generated report includes:

### 1. **Tool Execution Summary**
A comprehensive table showing the status of all 12 analysis tools:

| Tool | Status | Details |
|------|--------|----------|
| ğŸ“ Structure | âœ… Pass | 140 files, 15 dirs |
| ğŸ”’ Security | âŒ Fail | 3 issues |
| âœ… Tests | âš ï¸ Error | Coverage: 9% |
| ğŸ­ Duplication | âŒ Fail | 78 duplicate blocks |

### 2. **Test Type Breakdown**
Automatically categorizes tests for better insight:

```markdown
**Test Types:**
- Unit: âœ… (15 files)
- Integration: âŒ (0 files)  
- E2E: âŒ (0 files)

ğŸ‘‰ **Recommendation:** Add integration tests to verify component interactions
```

### 3. **Recent Changes**
Shows last commit with full context:

```markdown
**Last Commit:** `3884c80` - Amit, 2 hours ago
*"feat: Reformat Git Status as Recent Changes section"*

**Status:** âš ï¸ 3 uncommitted change(s)
**Branch:** `main`
```

### 4. **Interactive Architecture Graph**
Clickable Mermaid.Live link with:
- Subgraph grouping by directory
- Color-coded dependencies
- Pako compression for long graphs

---

## ğŸ“ Project Structure

```
mcp-python-auditor/
â”œâ”€â”€ app/                  # Application source code (if using FastAPI mode)
â”‚   â”œâ”€â”€ agents/          # Analyzer orchestration
â”‚   â”œâ”€â”€ core/            # Base classes, config
â”‚   â””â”€â”€ tools/           # 12 analysis tool implementations
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ MCP_USER_GUIDE.md           # Setup guide for Claude/Cursor
â”‚   â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md   # Technical details
â”‚   â”œâ”€â”€ AUTOFIX_GUIDE.md            # Auto-fix workflow
â”‚   â””â”€â”€ SESSION_SUMMARY.md          # Development history
â”œâ”€â”€ backups/             # Backup files and logs
â”œâ”€â”€ reports/             # Generated audit reports
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ data/                # Training datasets
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ mcp_fastmcp_server.py   # â­ Main MCP entry point
â”œâ”€â”€ verify_tools.py          # Tool verification script
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Development

### Running Tests
```bash
pytest tests/ -v --cov=app
```

### Verifying Tools
```bash
python verify_tools.py
```

Output:
```
ğŸ” Verifying Audit Tools Installation
==================================================
âœ… bandit               - Security scanning
âœ… detect-secrets       - Secret detection
âœ… vulture              - Dead code detection
...
ğŸ‰ All tools are properly installed!
```

### Adding a New Tool
1. Create `app/tools/your_tool.py` inheriting from `BaseTool`
2. Implement `analyze(project_path: Path) -> dict`
3. Register in `app/core/tool_registry.py`
4. Add to parallel execution in `mcp_fastmcp_server.py`

### Docker Setup
```bash
docker-compose up --build
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [MCP User Guide](docs/MCP_USER_GUIDE.md) | How to configure and use with AI assistants |
| [Implementation Summary](docs/IMPLEMENTATION_SUMMARY.md) | Technical architecture details |
| [Auto-Fix Guide](docs/AUTOFIX_GUIDE.md) | Safe code cleanup workflow |
| [Session Summary](docs/SESSION_SUMMARY.md) | Development history |

---

## ğŸ› Troubleshooting

### "Tool not found" in Claude
- **Fix:** Restart Claude Desktop
- **Check:** Look at logs in `%APPDATA%\Claude\logs\`

### "Missing tool: bandit"
- **Fix Option 1:** Let AI install them (agentic flow)
  ```
  User: "Run audit"
  AI: "Tools missing. Install them?"
  User: "Yes"
  ```
- **Fix Option 2:** Manual install
  ```bash
  pip install bandit detect-secrets vulture radon ruff pip-audit
  ```
- **Verify:** `python verify_tools.py`

### False Scores (e.g., 90/100 with 9% coverage)
- **Cause:** Using old version before scoring fix
- **Fix:** Pull latest code: `git pull origin main`
- **New behavior:** Realistic scores (45/100 for that project)

### Timeout errors
- **Cause:** Large codebases or slow disk I/O
- **Fix:** Tools have built-in timeouts (60s-300s) and will gracefully fail

---

## ğŸ‰ Recent Improvements

**v2.2 - Major Quality Update (Jan 2026)**
- âœ… Fixed scoring algorithm (now realistic, not inflated)
- âœ… Added agentic dependency installation
- âœ… Tool execution summary in reports
- âœ… Test type breakdown (Unit/Integration/E2E)
- âœ… Recent changes section with last commit
- âœ… Comprehensive tool verification script

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new tools
4. Run `python verify_tools.py` before committing
5. Submit a PR with clear description

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- Built with [FastMCP](https://github.com/jlowin/fastmcp) by Marvin/Prefect
- AST parsing inspired by [Bandit](https://github.com/PyCQA/bandit)
- Report structure based on industry code review standards
- Scoring algorithm designed for honest project assessment

---

**Made with â¤ï¸ for clean, secure Python codebases**

*No more false positives. No more silent failures. Just honest, actionable insights.*
